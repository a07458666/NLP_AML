{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_DataPreprocessing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+Fv3c9JXJMO1Y/gZ1aia+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a07458666/TBrainNLP/blob/master/1_DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDBApJJNXgK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFm2fmkXzAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir(r\"/content/drive/My Drive/tbrainData/\")\n",
        "# 讀入官方的data\n",
        "newsDf = pd.read_csv('train_final.csv')\n",
        "newsDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhhAf44JYK3F",
        "colab_type": "text"
      },
      "source": [
        "## 文章前處理\n",
        "刪除 英文 數字 特殊符號"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGbIg6-oYXbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newsDf[\"article\"] = newsDf[\"article\"].apply(lambda x: re.sub(\"[-／〔〕【】（） 「」()：:0-9A-Za-z\\n\\r//、.%，▲◆▪<>#*○]\", \"\", str(x)))\n",
        "newsDf[\"article\"] = newsDf[\"article\"].apply(lambda x: re.sub(\"[!！？?]\", \"。\", str(x)))\n",
        "\n",
        "newsDf.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhUHwIg5rH3D",
        "colab_type": "text"
      },
      "source": [
        "## 將文章分成句子\n",
        "因為文章太長 不可能一次進入模型\n",
        "所以一個句子一個句子預測名字"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gLLd8ZPx4ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "def BIO_encode(content, names):\n",
        "    if str(names) == \"['']\": \n",
        "      Context_list, BIO_list = split(content)+[''], np.repeat('O', len(content)+1).tolist()\n",
        "    else:\n",
        "\n",
        "      start_idx = []\n",
        "      end_idx = []\n",
        "      name_list = []\n",
        "      NER_list = []\n",
        "\n",
        "      for name in names:\n",
        "          # name = truth[0]\n",
        "          name_start = [m.start() for m in re.finditer(name, content)]\n",
        "          name_end = list(np.asarray(name_start) + len(name))\n",
        "\n",
        "          # 紀錄entity & 名字名稱\n",
        "          NER_list += np.repeat('PERSON', len(name_start)).tolist()\n",
        "          name_list += np.repeat(name, len(name_start)).tolist()\n",
        "          \n",
        "          # 名字在文章中的start, end\n",
        "          start_idx += name_start\n",
        "          end_idx += name_end\n",
        "\n",
        "      # 列出剛剛找到的entity在文章中的資訊\n",
        "      print('zip ', start_idx, end_idx, NER_list, name_list)\n",
        "      entity_info_list = [set(zip(start_idx, end_idx, NER_list, name_list))]\n",
        "      if str(entity_info_list) == '[set()]':\n",
        "        Context_list, BIO_list = split(content)+[''], np.repeat('O', len(content)+1).tolist()\n",
        "        return Context_list, BIO_list\n",
        "\n",
        "      print('entity_info_list ', entity_info_list)\n",
        "\n",
        "      review_PER = []\n",
        "      for entity in sorted(entity_info_list[0]):\n",
        "          # print(entity)\n",
        "          if entity[2] == 'PERSON':\n",
        "              review_PER.append(entity[3])\n",
        "\n",
        "      Context_list=[]\n",
        "      BIO_list = []\n",
        "      sorted_entity_info_list = sorted(entity_info_list[0])\n",
        "\n",
        "      for i, entity_info in enumerate(sorted_entity_info_list):\n",
        "          word_start_loc, word_end_loc, flag, word = entity_info\n",
        "          # 補O\n",
        "          if word_start_loc != 0:\n",
        "              if i != 0:\n",
        "                  O_start_loc = sorted(sorted_entity_info_list)[i-1][1]\n",
        "              else:\n",
        "                  O_start_loc = 0\n",
        "\n",
        "              O_end_loc = word_start_loc\n",
        "              add_O_string = content[O_start_loc:O_end_loc]\n",
        "              sub_split_words = split(add_O_string)\n",
        "              print(sub_split_words)\n",
        "              sub_BIO = np.repeat('O', len(add_O_string)).tolist()\n",
        "\n",
        "              Context_list += sub_split_words\n",
        "              BIO_list += sub_BIO\n",
        "          \n",
        "          # 當圈人名\n",
        "          sub_split_words = split(word)\n",
        "\n",
        "          # 人名\n",
        "          if (flag == 'PERSON') or (flag == 'ORG') or (flag == 'GPE'):\n",
        "\n",
        "              # 人名\n",
        "              if (flag == 'PERSON'):\n",
        "                  flag2 = 'PER'\n",
        "              # 機構名\n",
        "              if (flag == 'ORG'):\n",
        "                  flag2 = 'ORG'\n",
        "              # 地名\n",
        "              if (flag == 'GPE'):\n",
        "                  flag2 = 'LOC'\n",
        "\n",
        "              if len(word) > 1:\n",
        "                  sub_BIO = ['B-' + flag2] + np.repeat('I-' + flag2, len(word) - 1).tolist()\n",
        "              else:\n",
        "                  sub_BIO = ['B-' + flag2]\n",
        "          else:\n",
        "              flag2 = 'O'\n",
        "              sub_BIO = np.repeat(flag2, len(word)).tolist()\n",
        "          \n",
        "          Context_list += sub_split_words\n",
        "          BIO_list += sub_BIO\n",
        "\n",
        "          # 上面處理完，但最後一個人名後面還有一堆字需要補O\n",
        "          if word_end_loc != len(content) and i == (len(sorted_entity_info_list)-1):\n",
        "              word = content[word_end_loc:]\n",
        "              sub_split_words = split(word)\n",
        "              sub_BIO = np.repeat('O', len(word)).tolist()\n",
        "\n",
        "              Context_list += sub_split_words\n",
        "              BIO_list += sub_BIO\n",
        "\n",
        "      Context_list.append('')\n",
        "      BIO_list.append('')\n",
        "    return Context_list, BIO_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwvc9g8rHjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_content = []\n",
        "all_BIO = []\n",
        "all_article_index = []\n",
        "for index, news_info in newsDf.iterrows():\n",
        "  '''\n",
        "  if index > 37:\n",
        "    break\n",
        "  if index < 37:\n",
        "    continue\n",
        "  '''\n",
        "  input_data = str(news_info['article'])\n",
        "  names = news_info['name'].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(', ')\n",
        "  if input_data != '' and len(input_data) > 2:\n",
        "    input_datas = input_data.split('。')\n",
        "    for input in input_datas:\n",
        "      if input != '':\n",
        "        print('index ', index,' names ', names,'\\n input_datas ', input)\n",
        "        context_list, BIO_list = BIO_encode(content = input, names = names)\n",
        "        all_content.append(context_list)\n",
        "        all_BIO.append(BIO_list)\n",
        "        all_article_index.append(index)\n",
        "        #print('index ', index,' content_lists ', context_list,'\\n BIO_lists ', BIO_list)\n",
        "        #for i in range(len(context_list)):\n",
        "        #  print('i = ', i, '  ', context_list[i], ' = ', BIO_list[i])\n",
        "        #print('len context_list= ', len(context_list), 'len BIO_list = ', len(BIO_list))\n",
        "\n",
        "print(all_content)\n",
        "print(all_BIO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSw7BLUqcAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('all_content', all_content)\n",
        "np.save('all_BIO', all_BIO)\n",
        "np.save('all_article_index', all_article_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQLjTuhRkw83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}