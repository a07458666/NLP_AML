{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_BERT_NER.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ6lk2VkAqzOIx0gljVtY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a07458666/TBrainNLP/blob/master/2_BERT_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocMaUipHe7J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kashgari==1.1.5\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install keras-applications==1.0.8\n",
        "!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQITeGlsfABw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(r\"/content/drive/My Drive/tbrainData/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkd3urPcfbKF",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmdehfokfgcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_content = np.load('all_content.npy', allow_pickle = True)\n",
        "all_BIO = np.load('all_BIO.npy', allow_pickle = True)\n",
        "all_article_index = np.load('all_article_index.npy', allow_pickle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qJgd7jnjmeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.DataFrame({'content': all_content, 'BIO': all_BIO, 'all_article_index': all_article_index})\n",
        "dataset[\"NoName\"] = dataset[\"BIO\"].apply(lambda x: int(all(np.array(x) == \"O\")))\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n0z3LDssrBC",
        "colab_type": "text"
      },
      "source": [
        "## split train / val / test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etOSWn9Zs245",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_validate_test_split(input_df, train_percent=.6, validate_percent=.2, seed=None):\n",
        "    input_list = np.unique(input_df['all_article_index']).tolist()\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(range(len(input_list))).tolist()\n",
        "    m = len(input_list)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = list(input_list[i] for i in perm[:train_end])\n",
        "    validate = list(input_list[i] for i in perm[train_end:validate_end])\n",
        "    test = list(input_list[i] for i in perm[validate_end:])\n",
        "        \n",
        "    # -- train data --\n",
        "    train_data = [dataset[dataset['all_article_index'].isin(train)]['content'].tolist(), dataset[dataset['all_article_index'].isin(train)]['BIO'].tolist(), dataset[dataset['all_article_index'].isin(train)]['all_article_index'].tolist()]\n",
        "\n",
        "    # -- valid data --\n",
        "    valid_data = [dataset[dataset['all_article_index'].isin(validate)]['content'].tolist(), dataset[dataset['all_article_index'].isin(validate)]['BIO'].tolist(), dataset[dataset['all_article_index'].isin(validate)]['all_article_index'].tolist()]\n",
        "\n",
        "    # -- test data --\n",
        "    test_data = [dataset[dataset['all_article_index'].isin(test)]['content'].tolist(),  dataset[dataset['all_article_index'].isin(test)]['BIO'].tolist(), dataset[dataset['all_article_index'].isin(test)]['all_article_index'].tolist()]\n",
        "\n",
        "    return train_data, valid_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvx9FFAPtV7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = train_validate_test_split(input_df=dataset, train_percent=.5, validate_percent=.3, seed=777)\n",
        "train_x, train_y, train_idx =  train_data[0], train_data[1], train_data[2]\n",
        "valid_x, valid_y, valid_idx =  valid_data[0], valid_data[1], valid_data[2]\n",
        "test_x, test_y, test_idx =  test_data[0], test_data[1], test_data[2]\n",
        "print(f\"train data count: {len(train_x)}\")\n",
        "print(f\"validate data count: {len(valid_x)}\")\n",
        "print(f\"test data count: {len(test_x)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouNbuetzudZC",
        "colab_type": "text"
      },
      "source": [
        "## Download BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeJp5kaGuQdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -nc \"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\"\n",
        "!unzip -o \"chinese_L-12_H-768_A-12.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noC1P0sRizYN",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8X2L4-_uoup",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MofK5UjlYTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.labeling import BiGRU_Model\n",
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "\n",
        "# 下载 BERT 权重\n",
        "\n",
        "bert_embedding = BERTEmbedding('chinese_L-12_H-768_A-12',\n",
        "                               task=kashgari.LABELING,\n",
        "                               sequence_length=128)\n",
        "model = BiGRU_Model(bert_embedding)\n",
        "\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "model.fit(train_x, train_y, valid_x, valid_y, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}